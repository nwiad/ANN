########################
# Additional Files
########################
# cnn_drop_before_relu
# run_cnn.sh
# plot.py
# run_mlp.ipynb
# .DS_Store
# README.md
# cnn_relu_before_bn
# run_cnn.ipynb
# run_mlp.sh
# test_gpu_colab.ipynb

########################
# Filled Code
########################
# ../codes/mlp/model.py:1
        self.momentum = 0.9
        self.epsilon = 1e-5
        self.weight = Parameter(torch.ones(num_features))
        self.bias = Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))

        if self.training:
            mean = torch.mean(input, dim=0, keepdim=True)
            var = torch.var(input, dim=0, keepdim=True)
            self.running_mean = self.running_mean * self.momentum + mean * (1 - self.momentum)
            self.running_var = self.running_var * self.momentum + var * (1 - self.momentum)
            shifted_input = (input - mean) / torch.sqrt(var + self.epsilon)
        else:
            shifted_input = (input - self.running_mean) / torch.sqrt(self.running_var + self.epsilon)
        return shifted_input * self.weight + self.bias

# ../codes/mlp/model.py:2
        if self.training:
            return 1 / (1 - self.p) * input * torch.bernoulli(torch.ones_like(input) * (1 - self.p))
        else:
            return input

# ../codes/mlp/model.py:3
        self.hidden_dim = 1200
        self.batch_norm = batch_norm
        self.dropout = dropout
        self.fc1 = nn.Linear(3 * 32 * 32, self.hidden_dim)
        if batch_norm:
            self.bn = BatchNorm1d(self.hidden_dim)
        self.relu = nn.ReLU()
        if dropout:
            self.dropout = Dropout(drop_rate)
        self.fc2 = nn.Linear(self.hidden_dim, 10)

# ../codes/mlp/model.py:4
        x = self.fc1(x)
        if self.batch_norm:
            x = self.bn(x)
        x = self.relu(x)
        if self.dropout:
            x = self.dropout(x)
        logits = self.fc2(x)

# ../codes/cnn/model.py:1
        super(BatchNorm2d, self).__init__()
        self.momentum = 0.9
        self.epsilon = 1e-5
        self.weight = Parameter(torch.ones(num_features))
        self.bias = Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        # input: [batch_size, num_feature_map, height, width]
        if self.training:
            mean = torch.mean(input, dim=(0, 2, 3), keepdim=True)
            var = torch.var(input, dim=(0, 2, 3), keepdim=True)
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean.squeeze()
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var.squeeze()
            shifted_input = (input - mean) / torch.sqrt(var + self.epsilon)
        else:
            shifted_input = (input - self.running_mean.unsqueeze(-1).unsqueeze(-1)) / torch.sqrt(self.running_var.unsqueeze(-1).unsqueeze(-1) + self.epsilon)
        return shifted_input * self.weight.unsqueeze(-1).unsqueeze(-1) + self.bias.unsqueeze(-1).unsqueeze(-1)

# ../codes/cnn/model.py:2
        # input: [batch_size, num_feature_map, height, width]
        if self.training:
            # randomly zero out some feature maps (input.shape[1]) instead of some neurons
            return 1 / (1 - self.p) * input * torch.bernoulli(torch.ones(input.shape[0], input.shape[1], device=input.device) * (1 - self.p)).unsqueeze(-1).unsqueeze(-1)
        else:
            return input

# ../codes/cnn/model.py:3
        self.out_channels = [64, 64]
        self.kernel_size = [3, 3]
        self.maxpool_size = [2, 2]
        self.batch_norm = batch_norm
        self.dropout = dropout
        self.conv1 = nn.Conv2d(3, self.out_channels[0], self.kernel_size[0], padding=(self.kernel_size[0] - 1) // 2)
        if batch_norm:
            self.bn1 = BatchNorm2d(self.out_channels[0])
            self.bn2 = BatchNorm2d(self.out_channels[1])
        if dropout:
            self.dropout1 = Dropout(drop_rate)
            self.dropout2 = Dropout(drop_rate)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(self.maxpool_size[0], stride=self.maxpool_size[0])
        self.conv2 = nn.Conv2d(self.out_channels[0], self.out_channels[1], self.kernel_size[1], padding=(self.kernel_size[1] - 1) // 2)
        self.relu2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(self.maxpool_size[1], stride=self.maxpool_size[1])
        self.fc = nn.Linear(32//self.maxpool_size[0]//self.maxpool_size[1]*32//self.maxpool_size[0]//self.maxpool_size[1]*self.out_channels[1], 10)

# ../codes/cnn/model.py:4
        x = self.conv1(x)
        if self.batch_norm:
            x = self.bn1(x)
        x = self.relu1(x)
        if self.dropout:
            x = self.dropout1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        if self.batch_norm:
            x = self.bn2(x)
        x = self.relu2(x)
        if self.dropout:
            x = self.dropout2(x)
        x = self.maxpool2(x)
        x = x.view(x.shape[0], -1)
        logits = self.fc(x)


########################
# References
########################

########################
# Other Modifications
########################
# _codes/mlp/main.py -> ../codes/mlp/main.py
# 14 + import wandb
# 15 +
# 16 + axis = []
# 17 + train_loss_list, train_acc_list = [], []
# 18 + val_loss_list, val_acc_list = [], []
# 19 + final_test_acc = 0.0
# 39 + parser.add_argument('--batch_norm', type=int, default=1,
# 40 +     help='Whether to use batch normalization. Default: 1')
# 41 + parser.add_argument('--dropout', type=int, default=1,
# 42 +                     help='Whether to use dropout. Default: 1')
# 44 +
# 45 + print(f"batch_size={args.batch_size}, drop_rate={args.drop_rate}")
# 46 + wandb.init(
# 47 +     project="mlp",
# 48 +     config={
# 49 +         "num_epochs": args.num_epochs,
# 50 +         "batch_size": args.batch_size,
# 51 +         "drop_rate": args.drop_rate,
# 52 +         "learning_rate": args.learning_rate,
# 53 +         "batch_norm": args.batch_norm,
# 54 +         "dropout": args.dropout
# 55 +     },
# 56 +     name=f'mlp_{args.batch_size}_{args.drop_rate}_{args.learning_rate}'
# 57 + )
# 108 -         mlp_model = Model(drop_rate=drop_rate)
# 132 +         mlp_model = Model(drop_rate=args.drop_rate, batch_norm=args.batch_norm, dropout=args.dropout)
# 176 +             axis.append(epoch)
# 177 +             train_acc_list.append(train_acc)
# 178 +             train_loss_list.append(train_loss)
# 179 +             val_acc_list.append(val_acc)
# 180 +             val_loss_list.append(val_loss)
# 181 +             final_test_acc = test_acc
# 182 +
# 183 +             wandb.log({
# 184 +                 "train_acc": train_acc,
# 185 +                 "train_loss": train_loss,
# 186 +                 "val_acc": val_acc,
# 187 +                 "val_loss": val_loss
# 188 +             })
# 189 +
# 206 +
# 207 +     from plot import plot
# 208 +     plot(axis, train_acc_list, train_loss_list, val_acc_list, val_loss_list, args.batch_size, args.drop_rate, args.learning_rate)
# 209 +     with open("final_test_acc.txt", "a") as f:
# 210 +         f.write("batch_size: {}, drop_rate: {}, learning_rate: {}, final_test_acc: {}\n".format(args.batch_size, args.drop_rate, args.learning_rate, final_test_acc))
# _codes/mlp/model.py -> ../codes/mlp/model.py
# 40 -     def __init__(self, drop_rate=0.5):
# 54 +     def __init__(self, drop_rate=0.5, batch_norm=True, dropout=True):
# _codes/cnn/main.py -> ../codes/cnn/main.py
# 14 +
# 15 + import wandb
# 16 +
# 17 + axis = []
# 18 + train_loss_list, train_acc_list = [], []
# 19 + val_loss_list, val_acc_list = [], []
# 20 + final_test_acc = 0.0
# 40 + parser.add_argument('--batch_norm', type=int, default=1,
# 41 +     help='Whether to use batch normalization. Default: 1')
# 42 + parser.add_argument('--dropout', type=int, default=1,
# 43 +     help='Whether to use dropout. Default: 1')
# 46 + print(f"batch_size={args.batch_size}, drop_rate={args.drop_rate}")
# 47 + wandb.init(
# 48 +     project="cnn",
# 49 +     config={
# 50 +         "num_epochs": args.num_epochs,
# 51 +         "batch_size": args.batch_size,
# 52 +         "drop_rate": args.drop_rate,
# 53 +         "learning_rate": args.learning_rate,
# 54 +         "batch_norm": args.batch_norm,
# 55 +         "dropout": args.dropout
# 56 +     },
# 57 +     name=f'cnn_{args.batch_size}_{args.drop_rate}_{args.learning_rate}'
# 58 + )
# 108 -         cnn_model = Model(drop_rate=args.drop_rate)
# 132 +         cnn_model = Model(drop_rate=args.drop_rate, batch_norm=args.batch_norm, dropout=args.dropout)
# 176 +             axis.append(epoch)
# 177 +             train_acc_list.append(train_acc)
# 178 +             train_loss_list.append(train_loss)
# 179 +             val_acc_list.append(val_acc)
# 180 +             val_loss_list.append(val_loss)
# 181 +             final_test_acc = test_acc
# 182 +
# 183 +             wandb.log({
# 184 +                 "train_acc": train_acc,
# 185 +                 "train_loss": train_loss,
# 186 +                 "val_acc": val_acc,
# 187 +                 "val_loss": val_loss
# 188 +             })
# 189 +
# 207 +
# 208 +     from plot import plot
# 209 +     plot(axis, train_acc_list, train_loss_list, val_acc_list, val_loss_list, args.batch_size, args.drop_rate, args.learning_rate)
# 210 +     with open("final_test_acc.txt", "a") as f:
# 211 +         f.write("batch_size: {}, drop_rate: {}, learning_rate: {}, final_test_acc: {}\n".format(args.batch_size, args.drop_rate, args.learning_rate, final_test_acc))
# _codes/cnn/model.py -> ../codes/cnn/model.py
# 7 - class BatchNorm1d(nn.Module):
# 7 ?                ^
# 7 + class BatchNorm2d(nn.Module):
# 7 ?                ^
# 40 -     def __init__(self, drop_rate=0.5):
# 54 +     def __init__(self, drop_rate=0.5, batch_norm=True, dropout=True):
# 47 -     def forward(self, x, y=None):
# 79 +     def forward(self, x, y=None):
# 79 ?                                  +

